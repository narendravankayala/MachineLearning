{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2 - Classification\n",
    "\n",
    "<font color=\"red\"> <b> Due: Mar 5 (Tuesday) 11:00 pm </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Narendra Kumar Vankayala </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "The following notebook refers to four different classification algorithms descriptions and implementations\n",
    "* Pocket Algorithms\n",
    "* Quadratic Discriminant Analysis(QDA)\n",
    "* Liner Discriminant Analysis(LDA)\n",
    "* Logistic Regression\n",
    "\n",
    "* This notebook also introduces different evaluation metrics such as Confusion matrix, Accuracy, Precision, Recall, F1-Score etc\n",
    "* I will also discuss the pros and cons of the above algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the article is popular or not by with a threshold on number of shares in social networks (popularity).\n",
    "* The articles were published by Mashable (www.mashable.com) \n",
    "* The dataset doesn't containt the content as the rights to reproduce it belongs to them.\n",
    "* The dataset does not share the original content but some statistics associated with it. \n",
    "* The original content can be publicly accessed and retrieved using the provided urls in the dataset. \n",
    "* The dataset is aquired on January 8, 2015 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the dataset\n",
    "* The dataset \"OnlineNewsPopoularity\" contains has 39797 samples of the news articles and 61 features ( 58 predictive , 2 non-predictive , 1 goal field( No. of Shares))\n",
    "    0. url: URL of the article (non-predictive) \n",
    "    1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) \n",
    "    2. n_tokens_title: Number of words in the title \n",
    "    3. n_tokens_content: Number of words in the content \n",
    "    4. n_unique_tokens: Rate of unique words in the content \n",
    "    5. n_non_stop_words: Rate of non-stop words in the content \n",
    "    6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content \n",
    "    7. num_hrefs: Number of links \n",
    "    8. num_self_hrefs: Number of links to other articles published by Mashable \n",
    "    9. num_imgs: Number of images \n",
    "    10. num_videos: Number of videos \n",
    "    11. average_token_length: Average length of the words in the content \n",
    "    12. num_keywords: Number of keywords in the metadata \n",
    "    13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? \n",
    "    14. data_channel_is_entertainment: Is data channel 'Entertainment'? \n",
    "    15. data_channel_is_bus: Is data channel 'Business'? \n",
    "    16. data_channel_is_socmed: Is data channel 'Social Media'? \n",
    "    17. data_channel_is_tech: Is data channel 'Tech'? \n",
    "    18. data_channel_is_world: Is data channel 'World'? \n",
    "    19. kw_min_min: Worst keyword (min. shares) \n",
    "    20. kw_max_min: Worst keyword (max. shares) \n",
    "    21. kw_avg_min: Worst keyword (avg. shares) \n",
    "    22. kw_min_max: Best keyword (min. shares) \n",
    "    23. kw_max_max: Best keyword (max. shares) \n",
    "    24. kw_avg_max: Best keyword (avg. shares) \n",
    "    25. kw_min_avg: Avg. keyword (min. shares) \n",
    "    26. kw_max_avg: Avg. keyword (max. shares) \n",
    "    27. kw_avg_avg: Avg. keyword (avg. shares) \n",
    "    28. self_reference_min_shares: Min. shares of referenced articles in Mashable \n",
    "    29. self_reference_max_shares: Max. shares of referenced articles in Mashable \n",
    "    30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable \n",
    "    31. weekday_is_monday: Was the article published on a Monday? \n",
    "    32. weekday_is_tuesday: Was the article published on a Tuesday? \n",
    "    33. weekday_is_wednesday: Was the article published on a Wednesday? \n",
    "    34. weekday_is_thursday: Was the article published on a Thursday? \n",
    "    35. weekday_is_friday: Was the article published on a Friday? \n",
    "    36. weekday_is_saturday: Was the article published on a Saturday? \n",
    "    37. weekday_is_sunday: Was the article published on a Sunday? \n",
    "    38. is_weekend: Was the article published on the weekend? \n",
    "    39. LDA_00: Closeness to LDA topic 0 \n",
    "    40. LDA_01: Closeness to LDA topic 1 \n",
    "    41. LDA_02: Closeness to LDA topic 2 \n",
    "    42. LDA_03: Closeness to LDA topic 3 \n",
    "    43. LDA_04: Closeness to LDA topic 4 \n",
    "    44. global_subjectivity: Text subjectivity \n",
    "    45. global_sentiment_polarity: Text sentiment polarity \n",
    "    46. global_rate_positive_words: Rate of positive words in the content \n",
    "    47. global_rate_negative_words: Rate of negative words in the content \n",
    "    48. rate_positive_words: Rate of positive words among non-neutral tokens \n",
    "    49. rate_negative_words: Rate of negative words among non-neutral tokens \n",
    "    50. avg_positive_polarity: Avg. polarity of positive words \n",
    "    51. min_positive_polarity: Min. polarity of positive words \n",
    "    52. max_positive_polarity: Max. polarity of positive words \n",
    "    53. avg_negative_polarity: Avg. polarity of negative words \n",
    "    54. min_negative_polarity: Min. polarity of negative words \n",
    "    55. max_negative_polarity: Max. polarity of negative words \n",
    "    56. title_subjectivity: Title subjectivity \n",
    "    57. title_sentiment_polarity: Title polarity \n",
    "    58. abs_title_subjectivity: Absolute subjectivity level \n",
    "    59. abs_title_sentiment_polarity: Absolute polarity level \n",
    "    60. shares: Number of shares (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of the data\n",
    "* The dataset is taken from University of California, Irvine - Machine Learning Repository,Center for Machine Learning and Intelligent Systems\n",
    "* The creators of the dataset are Kelwin Fernandes (kafc ‘@’ inesctec.pt, kelwinfc ’@’ gmail.com), Pedro Vinagre (pedro.vinagre.sousa ’@’ gmail.com) and Pedro Sernadela\n",
    "* The dataset can be found at https://archive.ics.uci.edu/ml/datasets/online+news+popularity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcol\n",
    "from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs   ...     \\\n",
       "0                   0.815385         4.0              2.0        1.0   ...      \n",
       "1                   0.791946         3.0              1.0        1.0   ...      \n",
       "2                   0.663866         3.0              1.0        1.0   ...      \n",
       "3                   0.665635         9.0              0.0        1.0   ...      \n",
       "4                   0.540890        19.0             19.0       20.0   ...      \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Pandas Python library we read the csv file stored in the directory same as this jupyter notebook\n",
    "df = pd.read_csv(\"OnlineNewsPopularity.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['url', ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
       "       ' n_unique_tokens', ' n_non_stop_words',\n",
       "       ' n_non_stop_unique_tokens', ' num_hrefs', ' num_self_hrefs',\n",
       "       ' num_imgs', ' num_videos', ' average_token_length',\n",
       "       ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min',\n",
       "       ' kw_avg_min', ' kw_min_max', ' kw_max_max', ' kw_avg_max',\n",
       "       ' kw_min_avg', ' kw_max_avg', ' kw_avg_avg',\n",
       "       ' self_reference_min_shares', ' self_reference_max_shares',\n",
       "       ' self_reference_avg_sharess', ' weekday_is_monday',\n",
       "       ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday',\n",
       "       ' weekday_is_saturday', ' weekday_is_sunday', ' is_weekend',\n",
       "       ' LDA_00', ' LDA_01', ' LDA_02', ' LDA_03', ' LDA_04',\n",
       "       ' global_subjectivity', ' global_sentiment_polarity',\n",
       "       ' global_rate_positive_words', ' global_rate_negative_words',\n",
       "       ' rate_positive_words', ' rate_negative_words',\n",
       "       ' avg_positive_polarity', ' min_positive_polarity',\n",
       "       ' max_positive_polarity', ' avg_negative_polarity',\n",
       "       ' min_negative_polarity', ' max_negative_polarity',\n",
       "       ' title_subjectivity', ' title_sentiment_polarity',\n",
       "       ' abs_title_subjectivity', ' abs_title_sentiment_polarity',\n",
       "       ' shares'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(df.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39644, 61)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['popularity'] = np.where(df['shares'] > 1400 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated  = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.drop(['shares','url','timedelta'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg',\n",
       "       'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares',\n",
       "       'self_reference_max_shares', 'self_reference_avg_sharess',\n",
       "       'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday',\n",
       "       'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday',\n",
       "       'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02',\n",
       "       'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'popularity'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = df_updated['popularity'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_updated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg',\n",
       "       'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares',\n",
       "       'self_reference_max_shares', 'self_reference_avg_sharess',\n",
       "       'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday',\n",
       "       'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday',\n",
       "       'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02',\n",
       "       'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop(['popularity'], axis=1, inplace=True)\n",
    "X.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39644, 58), (39644,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Method\n",
    "\n",
    "Summarize the pocket algorithm, discriminant analysis, and logistic regression.\n",
    "The superclass *Classifier* defines common utility methods. \n",
    "Finish the normalize function for you. \n",
    "Do not forget explain your implementation. \n",
    "\n",
    "The explanation of your codes should not be the comments in a code cell. \n",
    "This section should include\n",
    " - review of the 4 classification models \n",
    " - your implementation and description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Super Classs Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Super class for machine learning models \n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "class Classifier(BaseModel):\n",
    "    \"\"\"\n",
    "        Abstract class for classification \n",
    "        \n",
    "        Attributes\n",
    "        ==========\n",
    "        meanX       ndarray\n",
    "                    mean of inputs (from standardization)\n",
    "        stdX        ndarray\n",
    "                    standard deviation of inputs (standardization)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        self.meanX = None\n",
    "        self.stdX = None\n",
    "\n",
    "    def normalize(self, X):\n",
    "        \"\"\" standardize the input X \"\"\"\n",
    "        \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.asanyarray(X)\n",
    "\n",
    "        self.meanX = np.mean(X, 0)\n",
    "        self.stdX = np.std(X, 0)\n",
    "\n",
    "        # TODO: Finish this normalization\n",
    "        \n",
    "        Xs = np.divide(X-self.meanX, self.stdX)\n",
    "        return Xs\n",
    "\n",
    "    def _check_matrix(self, mat, name):\n",
    "        if len(mat.shape) != 2:\n",
    "            raise ValueError(''.join([\"Wrong matrix \", name]))\n",
    "        \n",
    "    # add a basis\n",
    "    def add_ones(self, X):\n",
    "        \"\"\"\n",
    "            add a column basis to X input matrix\n",
    "        \"\"\"\n",
    "        self._check_matrix(X, 'X')\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    ####################################################\n",
    "    #### abstract funcitons ############################\n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Pocket Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "class pocketAlgorithm(Classifier):\n",
    "    \n",
    "    def __init__(self, alpha, maxIterations):\n",
    "        self.alpha = 0.1\n",
    "        self.maxIterations = 5\n",
    "        self.w = None\n",
    "        self.wpocket= None\n",
    "        \n",
    "    \n",
    "    def bestWeight(self, X, T):\n",
    "        ycurrentweight = np.sign(X @ self.w)\n",
    "        ypocketweight  = np.sign(X @ self.wpocket)\n",
    "        return np.sum(ycurrentweight == T) > np.sum(ypocketweight == T)\n",
    "    \n",
    "    def train(self, X , T):\n",
    "        X = self.add_ones(X)\n",
    "        self.w = np.ones(X.shape[1])\n",
    "        self.wpocket = np.ones(X.shape[1])\n",
    "        for i in range(self.maxIterations):\n",
    "            converge = True\n",
    "            for j in range(X.shape[0]):\n",
    "                y = self.w @ X[j]\n",
    "                if np.sign(y) != np.sign(T[j]):\n",
    "                    self.w += self.alpha * T[j] * X[j]\n",
    "                    converge = False\n",
    "                    if self.bestWeight(X, T):\n",
    "                        print(\"updating weight\")\n",
    "                        self.wpocket = copy(self.w)\n",
    "            if converge:\n",
    "                print(\"Converged at iteration \" , i )\n",
    "                break\n",
    "    def use(self, X):\n",
    "        X = self.add_ones(X)\n",
    "        return X @ self.wpocket\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class qda(Classifier):\n",
    "    def train(self , X, T):\n",
    "        self.classes = list(Counter(T).keys())\n",
    "        self.total   = T.shape[0]\n",
    "        self.prior = dict((key, Counter(T)[key]/self.total) for key in self.classes)\n",
    "        X = self.normalize(X)\n",
    "        \n",
    "        self.cov = dict((key, None) for key in self.classes)\n",
    "        for key in self.classes:\n",
    "            temp = np.array([ar for i,ar in enumerate(X) if T[i]==key])\n",
    "            cov = np.cov(temp.T)\n",
    "            self.cov.update({key:cov})\n",
    "        \n",
    "        self.mean = dict((key, None) for key in self.classes)\n",
    "        for key in self.classes:\n",
    "            temp = np.array([ar for i, ar in enumerate(X) if T[i]==key])\n",
    "            mu = np.mean(temp,0)\n",
    "            self.mean.update({key:mu})\n",
    "            \n",
    "    def discriminantFunction(self, X, key):\n",
    "        return -0.5 * np.log(np.linalg.det(np.abs(self.cov_[key]))) \\\n",
    "                -0.5 * np.sum(np.dot((X - self.mean[key]), np.linalg.inv(np.abs(self.cov_[key]))) * (X - self.mean[key]),axis=1) \\\n",
    "                + np.log(self.prior[key])\n",
    "    \n",
    "    def use(self, X):\n",
    "        result = []\n",
    "        for cls in self.classes:\n",
    "            result.append(self.discriminantFunction(X, c))\n",
    "        return list(map(lambda x:self.classes[x],np.argmax(list(zip(*result)),axis=1)))\n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lda(Classifier):\n",
    "    def train(self, X, T):\n",
    "        self.classes = list(Counter(T).keys())\n",
    "        self.total = T.shape[0]\n",
    "        self.prior = dict((key, Counter(T)[key]/self.total) for key in self.classes)\n",
    "        X = self.normalize(X)\n",
    "        self.cov = np.cov(X.T)\n",
    "        self.inversecov = np.linalg.inv(self.cov)\n",
    "        self.mean = dict((key, None) for key in self.classes)\n",
    "        for key in self.classes:\n",
    "            temp = np.array([ar for i, ar in enumerate(X) if T[i]==key])\n",
    "            mu = np.mean(temp,0)\n",
    "            self.mean.update({key:mu})\n",
    "            \n",
    "    def discriminantFunction(self, X, key):\n",
    "        return np.sum(np.dot(X,self.inversecov)*self.mean[key],axis=1)\\\n",
    "                - np.sum(0.5*np.dot(self.mean[key],self.inversecov)*self.mean[key])\\\n",
    "                + np.log(self.prior[key])\n",
    "    \n",
    "    def use(self, X):\n",
    "        result = []\n",
    "        for cls in self.classes:\n",
    "            result.append(self.discriminantFunction(X, cls))\n",
    "        return list(map(lambda x: self.classes[x], np.argmax(list(zip(*result)),axis= 1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Classifier):\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.alpha = 0.1\n",
    "        self.iter = 10\n",
    "        \n",
    "    def softmax(self, X):\n",
    "        epowerx = np.exp(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return epowerx/np.sum(epowerx)\n",
    "        else:\n",
    "            return epowerx/np.sum(epowerx, axis = 1, keepdims=True)\n",
    "    \n",
    "    def fofX(self, X):\n",
    "        return self.softmax(X@self.w)\n",
    "    \n",
    "    def train(self, X, T):\n",
    "        X =self.add_ones(X)\n",
    "        T = pd.get_dummies(T)\n",
    "        self.classes = T.column.values\n",
    "        self.w = np.random.rand(X.shape[0], T.shape[0])*2 -1 \n",
    "        for k in range(self.iter):\n",
    "            y = self.fofX(X)\n",
    "            self.w +=self.alpha * X.T @(T-y)\n",
    "    \n",
    "    def use(self, X):\n",
    "        X = self.add_ones(X)\n",
    "        return [self.classes[i] for i in np.argmax(self.fofX(X), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true,predicted):\n",
    "    return np.sum(predicted == true)*100/true.shape[0]\n",
    "\n",
    "def confusion_matrix(true,predicted):\n",
    "    import pandas as pd\n",
    "    cf = pd.crosstab([true], [predicted], rownames = ['True'], colnames = ['Predicted'], margins = True)\n",
    "    return cf\n",
    "\n",
    "def F1_score(true,predicted):\n",
    "    c = confusion_matrix(true,predicted)\n",
    "    precision = c[1][1]/c[1]['All']\n",
    "    recall = c.loc[1][1]/c.loc[1]['All']\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_clf = pocketAlgorithm(alpha = 0.01, maxIterations = 10)\n",
    "pk_clf.train(X.values, T.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20082</td>\n",
       "      <td>20082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19562</td>\n",
       "      <td>19562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>39644</td>\n",
       "      <td>39644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    1.0    All\n",
       "True                   \n",
       "0          20082  20082\n",
       "1          19562  19562\n",
       "All        39644  39644"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op=np.sign(pk_clf.use(X.values))\n",
    "confusion_matrix(T,op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-f2624173a435>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-f2624173a435>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    Apply the classfiers on the data and discuss the results.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IV. Experiments\n",
    "\n",
    "Apply the classfiers on the data and discuss the results.\n",
    "Please describe your codes for experiments. You may have subsections of results and discussions here.\n",
    "Here follows the list that you consider to include:\n",
    "- the classification results\n",
    "- plots of classification results \n",
    "- model comparision \n",
    "- choice of evaluation metrics\n",
    "- **Must partition data into training and testing**\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "Summarize your work here. \n",
    "Which classifier do you think the best? \n",
    "Discuss the challenges or somethat that you learned. \n",
    "If you have any suggestion about the assignment, you can write about it. \n",
    "\n",
    "# References\n",
    "\n",
    "List all your references here.\n",
    "\n",
    "# Extra Credit\n",
    "\n",
    "* [OPT 1] Search for a ordinal data set and apply your classifiers to it. \n",
    "  - Repeat the experiments on it. \n",
    "  - Do you have different observation from previous results? \n",
    "  - Were you able to observe that we discussed in class about logistic regression? \n",
    "  - For a full extra credit point, you need to discuss all bullet points in Results section.     \n",
    "\n",
    "\n",
    "* [OPT 2] Partition your data into five sets. Selecting one test set and the other for training, repeat your experiments and observe/analyze the 5 different training/testing errors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "DO NOT forget to submit your data! Your notebook is supposed to run well after running your codes.\n",
    "\n",
    "To help our TA's grading, please make an explicit section for each grading criteria. \n",
    "Again, this is a **writing assignment**. Please don't forget to properly explain your codes and results using Markdown cell. \n",
    "\n",
    "\n",
    "points | | description\n",
    "--|--|:--\n",
    "5 | Overview| states the objective and the appraoch \n",
    "15 | Data | \n",
    " | 5| description \n",
    " | 5| plots for understanding or analysis \n",
    " | 5| preliminary observation \n",
    "25 | Methods | \n",
    " |10| Summary of Classification models\n",
    " | 5| Explanation of codes\n",
    " |10| Pocket, LDA, QDA, Logistic Regression\n",
    "40 | Experiments \n",
    "| 5| Discussion about evaluation metrics\n",
    "| 5| Discussion about train and test accuracies\n",
    "|20| plots for results (5 for each algorithm)\n",
    "|10| Discussions about classificaion model comparison\n",
    "5 | |Conclusions \n",
    "5 | |Referemces\n",
    "5 | |Grammar and spelling error (Proofread please)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
